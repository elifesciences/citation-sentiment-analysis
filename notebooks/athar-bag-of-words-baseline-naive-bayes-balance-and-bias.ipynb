{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Athar bag-of-words baseline naive-bayes - balance and bias\n",
    "\n",
    "This notebook explores the effect of different ways of balancing the dataset and train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from citation_sentiment_analysis.datasets.athar import (\n",
    "    download_and_read_athar_txt_with_sentiment_label,\n",
    "    filter_long_sentences_from_athar\n",
    ")\n",
    "from citation_sentiment_analysis.preprocessing.token_filter import (\n",
    "    get_default_words_to_include,\n",
    "    keep_sentence_list_tokens_in\n",
    ")\n",
    "from citation_sentiment_analysis.utils.jupyter import printmd\n",
    "from citation_sentiment_analysis.utils.scoring import train_test_score\n",
    "from citation_sentiment_analysis.utils.vectorizer import transform_to_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_paper_id</th>\n",
       "      <th>target_paper_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>citation_text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00-1043</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>o</td>\n",
       "      <td>We analyzed a set of articles and identified s...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H05-1033</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>o</td>\n",
       "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I05-2009</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>o</td>\n",
       "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I05-2009</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>o</td>\n",
       "      <td>(1999) proposed a summarization system based o...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I05-2009</td>\n",
       "      <td>A00-2024</td>\n",
       "      <td>o</td>\n",
       "      <td>We found that the deletion of lead parts did n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_paper_id target_paper_id sentiment  \\\n",
       "0        A00-1043        A00-2024         o   \n",
       "1        H05-1033        A00-2024         o   \n",
       "2        I05-2009        A00-2024         o   \n",
       "3        I05-2009        A00-2024         o   \n",
       "4        I05-2009        A00-2024         o   \n",
       "\n",
       "                                       citation_text sentiment_label  \n",
       "0  We analyzed a set of articles and identified s...         neutral  \n",
       "1  Table 3: Example compressions Compression AvgL...         neutral  \n",
       "2  5.3 Related works and discussion Our two-step ...         neutral  \n",
       "3  (1999) proposed a summarization system based o...         neutral  \n",
       "4  We found that the deletion of lead parts did n...         neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athar_all_df = filter_long_sentences_from_athar(download_and_read_athar_txt_with_sentiment_label())\n",
    "athar_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_by_target_paper_id(df):\n",
    "    df = df[df['sentiment'].isin(['n', 'o'])]\n",
    "    min_df = (\n",
    "        df.groupby(['target_paper_id', 'sentiment']).size()\n",
    "        .groupby(level=0).agg(lambda x: min(x) if len(x) == 2 else 0)\n",
    "    )\n",
    "    df = pd.concat([\n",
    "        df[df['target_paper_id'] == paper_id].groupby('sentiment').head(min_df[paper_id])\n",
    "        for paper_id in min_df.index\n",
    "    ])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_by_sentiment_only(df):\n",
    "    df_athar_negative = df[df['sentiment'] == \"n\"]\n",
    "    df_athar_neutral_selection = df[df['sentiment'] == \"o\"][:df_athar_negative.shape[0]]\n",
    "\n",
    "    return df_athar_negative.append(df_athar_neutral_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109442"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_include = get_default_words_to_include()\n",
    "\n",
    "len(words_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def athar_df_to_X_y(df):\n",
    "    citation_texts = df['citation_text']\n",
    "    citation_tokens = [nltk.word_tokenize(s) for s in citation_texts]\n",
    "    citation_filtered_tokens = keep_sentence_list_tokens_in(citation_tokens, words_to_include)\n",
    "    ps = PorterStemmer()\n",
    "    citation_stemmed_tokens = [[ps.stem(t) for t in tokens] for tokens in citation_filtered_tokens]\n",
    "    X = transform_to_counts(citation_stemmed_tokens)\n",
    "    y = df['sentiment'] == 'n'\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### train/test split\n",
       "* not randomised before undersampling\n",
       "* not randomised before train/test split\n",
       "* accuracy: **0.5357142857142857**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# regular train/test split no randomisation\n",
    "X, y = athar_df_to_X_y(balance_by_sentiment_only(athar_all_df))\n",
    "score = train_test_score(\n",
    "    BernoulliNB(), X, y, test_size=0.2,\n",
    "    scoring='accuracy',\n",
    "    shuffle=False\n",
    ")\n",
    "printmd('\\n'.join([\n",
    "    '### train/test split',\n",
    "    '* not randomised before undersampling',\n",
    "    '* not randomised before train/test split',\n",
    "    '* accuracy: **%s**'\n",
    "]) % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### train/test split\n",
       "* not randomised before undersampling\n",
       "* randomised before train/test split\n",
       "* accuracy: **0.8125**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# same as above, but randomise before train/test split\n",
    "X, y = athar_df_to_X_y(balance_by_sentiment_only(athar_all_df))\n",
    "score = train_test_score(\n",
    "    BernoulliNB(), X, y, test_size=0.2,\n",
    "    scoring='accuracy',\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "printmd('\\n'.join([\n",
    "    '### train/test split',\n",
    "    '* not randomised before undersampling',\n",
    "    '* randomised before train/test split',\n",
    "    '* accuracy: **%s**'\n",
    "]) % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### cross validation\n",
       "* not randomised before undersampling\n",
       "* not randomised before cv train/test split\n",
       "* accuracy: **0.780** (std: 0.054)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's do the same with cross validation, first without randomising\n",
    "X, y = athar_df_to_X_y(balance_by_sentiment_only(athar_all_df))\n",
    "scores = cross_val_score(\n",
    "    BernoulliNB(), X, y, cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "printmd('\\n'.join([\n",
    "    '### cross validation',\n",
    "    '* not randomised before undersampling',\n",
    "    '* not randomised before cv train/test split',\n",
    "    '* accuracy: **%.3f** (std: %.3f)'\n",
    "]) % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### cross validation\n",
       "* not randomised before undersampling\n",
       "* randomised before cv train/test split\n",
       "* accuracy: **0.857** (std: 0.020)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cross validation, with randomising before doing the splits\n",
    "X, y = athar_df_to_X_y(balance_by_sentiment_only(athar_all_df).sample(frac=1, random_state=42))\n",
    "scores = cross_val_score(\n",
    "    BernoulliNB(), X, y, cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "printmd('\\n'.join([\n",
    "    '### cross validation',\n",
    "    '* not randomised before undersampling',\n",
    "    '* randomised before cv train/test split',\n",
    "    '* accuracy: **%.3f** (std: %.3f)'\n",
    "]) % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### cross validation\n",
       "* special undersampling method: *by target paper*\n",
       "* not randomised before undersampling\n",
       "* randomised before cv train/test split\n",
       "* accuracy: **0.757** (std: 0.029)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try to balance also by target paper (i.e. get same number of neg/neutral citations from same paper)\n",
    "X, y = athar_df_to_X_y(balance_by_target_paper_id(athar_all_df).sample(frac=1, random_state=42))\n",
    "scores = cross_val_score(\n",
    "    BernoulliNB(), X, y, cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "printmd('\\n'.join([\n",
    "    '### cross validation',\n",
    "    '* special undersampling method: *by target paper*',\n",
    "    '* not randomised before undersampling',\n",
    "    '* randomised before cv train/test split',\n",
    "    '* accuracy: **%.3f** (std: %.3f)'\n",
    "]) % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### cross validation\n",
       "* randomised before undersampling\n",
       "* randomised before cv train/test split\n",
       "* accuracy: **0.784** (std: 0.020)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# go back to balancing by sentiment only, but randomise before doing so (and after)\n",
    "X, y = athar_df_to_X_y(balance_by_sentiment_only(\n",
    "    athar_all_df.sample(frac=1, random_state=42)\n",
    ").sample(frac=1, random_state=42))\n",
    "scores = cross_val_score(\n",
    "    BernoulliNB(), X, y, cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "printmd('\\n'.join([\n",
    "    '### cross validation',\n",
    "    '* randomised before undersampling',\n",
    "    '* randomised before cv train/test split',\n",
    "    '* accuracy: **%.3f** (std: %.3f)'\n",
    "]) % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### cross validation\n",
       "* no undersampling\n",
       "* randomised before cv train/test split\n",
       "* roc auc: **0.814** (std: 0.007)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now no undersampling, we're just using roc_auc as the scoring method instead\n",
    "X, y = athar_df_to_X_y(athar_all_df.sample(frac=1, random_state=42))\n",
    "scores = cross_val_score(\n",
    "    BernoulliNB(), X, y, cv=5,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "printmd('\\n'.join([\n",
    "    '### cross validation',\n",
    "    '* no undersampling',\n",
    "    '* randomised before cv train/test split',\n",
    "    '* roc auc: **%.3f** (std: %.3f)'\n",
    "]) % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Randomising the data is very important and most effective at the start.\n",
    "Additionally removing bias by only selecting target paper id produces similar result to just undersampling by class.\n",
    "\n",
    "The roc auc result suggest that there is potential to learn from more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
